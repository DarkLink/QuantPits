#!/usr/bin/env python
"""
Predict-Only è„šæœ¬ (Production Predict Only)
ä½¿ç”¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æœ€æ–°æ•°æ®è¿›è¡Œé¢„æµ‹å’Œå›æµ‹ï¼Œä¸é‡æ–°è®­ç»ƒã€‚

æ ¸å¿ƒè¯­ä¹‰ï¼š
- ä» latest_train_records.json è·å–å·²æœ‰æ¨¡å‹çš„ recorder_id
- åŠ è½½ model.pklï¼Œç”¨æœ€æ–°æ•°æ®ç”Ÿæˆæ–°é¢„æµ‹
- åœ¨ Prod_Predict_{Freq} å®éªŒä¸‹åˆ›å»ºæ–° Recorderï¼ˆå« pred.pkl + SignalRecordï¼‰
- ä»¥ merge æ–¹å¼æ›´æ–° latest_train_records.jsonï¼Œä¿è¯ä¸‹æ¸¸ç©·ä¸¾/èåˆå…¼å®¹

è¿è¡Œæ–¹å¼ï¼šcd QuantPits && python engine/scripts/prod_predict_only.py [options]

ç¤ºä¾‹ï¼š
  # é¢„æµ‹æ‰€æœ‰ enabled æ¨¡å‹
  python engine/scripts/prod_predict_only.py --all-enabled

  # é¢„æµ‹æŒ‡å®šæ¨¡å‹
  python engine/scripts/prod_predict_only.py --models gru,mlp

  # æŒ‰æ ‡ç­¾ç­›é€‰
  python engine/scripts/prod_predict_only.py --tag tree

  # Dry-runï¼ˆä»…æŸ¥çœ‹è®¡åˆ’ï¼‰
  python engine/scripts/prod_predict_only.py --models gru,mlp --dry-run

  # æŸ¥çœ‹å¯ç”¨æ¨¡å‹
  python engine/scripts/prod_predict_only.py --list
"""

import os
import sys
import json
import argparse
from datetime import datetime

import env
os.chdir(env.ROOT_DIR)

DEFAULT_EXPERIMENT_NAME = "Prod_Predict"


# å»¶è¿Ÿå¯¼å…¥ qlibï¼ˆåœ¨è§£æå‚æ•°ä¹‹åï¼‰
def init_qlib():
    """åˆå§‹åŒ– Qlib ç¯å¢ƒ"""
    import qlib
    from qlib.constant import REG_CN
    provider_uri = "~/.qlib/qlib_data/cn_data"
    qlib.init(provider_uri=provider_uri, region=REG_CN)


def parse_args():
    parser = argparse.ArgumentParser(
        description='ä»…é¢„æµ‹ï¼šä½¿ç”¨å·²æœ‰æ¨¡å‹å¯¹æœ€æ–°æ•°æ®è¿›è¡Œé¢„æµ‹å’Œå›æµ‹ï¼Œä¸é‡æ–°è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s --all-enabled                           # é¢„æµ‹æ‰€æœ‰ enabled æ¨¡å‹
  %(prog)s --models gru,mlp                        # é¢„æµ‹æŒ‡å®šæ¨¡å‹
  %(prog)s --algorithm lstm                        # é¢„æµ‹æ‰€æœ‰ LSTM ç³»åˆ—
  %(prog)s --tag tree                              # é¢„æµ‹æ‰€æœ‰ tree æ ‡ç­¾æ¨¡å‹
  %(prog)s --all-enabled --skip catboost_Alpha158  # è·³è¿‡æŒ‡å®šæ¨¡å‹
  %(prog)s --models gru --dry-run                  # ä»…æ‰“å°è®¡åˆ’ï¼Œä¸é¢„æµ‹
  %(prog)s --list                                  # åˆ—å‡ºæ‰€æœ‰æ³¨å†Œæ¨¡å‹
        """
    )

    # æ¨¡å‹é€‰æ‹©
    select = parser.add_argument_group('æ¨¡å‹é€‰æ‹©')
    select.add_argument('--models', type=str,
                        help='æŒ‡å®šæ¨¡å‹åï¼Œé€—å·åˆ†éš” (å¦‚: gru,mlp,alstm_Alpha158)')
    select.add_argument('--algorithm', type=str,
                        help='æŒ‰ç®—æ³•ç­›é€‰ (å¦‚: lstm, gru, lightgbm)')
    select.add_argument('--dataset', type=str,
                        help='æŒ‰æ•°æ®é›†ç­›é€‰ (å¦‚: Alpha158, Alpha360)')
    select.add_argument('--market', type=str,
                        help='æŒ‰å¸‚åœºç­›é€‰ (å¦‚: csi300)')
    select.add_argument('--tag', type=str,
                        help='æŒ‰æ ‡ç­¾ç­›é€‰ (å¦‚: ts, tree, attention)')
    select.add_argument('--all-enabled', action='store_true',
                        help='é¢„æµ‹æ‰€æœ‰ enabled=true çš„æ¨¡å‹')

    # æ’é™¤
    skip_group = parser.add_argument_group('æ’é™¤')
    skip_group.add_argument('--skip', type=str,
                            help='è·³è¿‡æŒ‡å®šæ¨¡å‹ï¼Œé€—å·åˆ†éš”')

    # æ•°æ®æ¥æº
    source = parser.add_argument_group('æ•°æ®æ¥æº')
    source.add_argument('--source-records', type=str,
                        default='latest_train_records.json',
                        help='æºè®­ç»ƒè®°å½•æ–‡ä»¶ï¼Œç”¨äºè·å–å·²æœ‰æ¨¡å‹ (é»˜è®¤: latest_train_records.json)')

    # è¿è¡Œæ§åˆ¶
    ctrl = parser.add_argument_group('è¿è¡Œæ§åˆ¶')
    ctrl.add_argument('--dry-run', action='store_true',
                      help='ä»…æ‰“å°å¾…é¢„æµ‹æ¨¡å‹åˆ—è¡¨ï¼Œä¸å®é™…æ‰§è¡Œ')
    ctrl.add_argument('--experiment-name', type=str,
                      default=DEFAULT_EXPERIMENT_NAME,
                      help=f'MLflow å®éªŒåç§° (é»˜è®¤: {DEFAULT_EXPERIMENT_NAME})')

    # ä¿¡æ¯æŸ¥çœ‹
    info = parser.add_argument_group('ä¿¡æ¯æŸ¥çœ‹')
    info.add_argument('--list', action='store_true',
                      help='åˆ—å‡ºæ¨¡å‹æ³¨å†Œè¡¨ï¼ˆå¯ç»“åˆç­›é€‰æ¡ä»¶ï¼‰')

    return parser.parse_args()


def resolve_target_models(args):
    """
    æ ¹æ® CLI å‚æ•°è§£æç›®æ ‡æ¨¡å‹åˆ—è¡¨

    Returns:
        dict: {model_name: model_info} æˆ– Noneï¼ˆæœªæŒ‡å®šé€‰æ‹©æ¡ä»¶ï¼‰
    """
    from train_utils import (
        load_model_registry,
        get_enabled_models,
        get_models_by_filter,
        get_models_by_names,
    )

    registry = load_model_registry()

    if args.models:
        model_names = [m.strip() for m in args.models.split(',')]
        targets = get_models_by_names(model_names, registry)
    elif args.all_enabled:
        targets = get_enabled_models(registry)
    elif args.algorithm or args.dataset or args.market or args.tag:
        targets = get_models_by_filter(
            registry,
            algorithm=args.algorithm,
            dataset=args.dataset,
            market=args.market,
            tag=args.tag
        )
    else:
        return None

    # åº”ç”¨ --skip
    if args.skip:
        skip_names = [m.strip() for m in args.skip.split(',')]
        targets = {k: v for k, v in targets.items() if k not in skip_names}
        if skip_names:
            print(f"â­ï¸  è·³è¿‡æ¨¡å‹: {', '.join(skip_names)}")

    return targets


def predict_single_model(model_name, model_info, params, experiment_name, source_records):
    """
    ä½¿ç”¨å·²æœ‰æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ï¼ˆä¸è®­ç»ƒï¼‰

    æµç¨‹ï¼š
    1. ä» source_records è·å–åŸå§‹ recorder_id
    2. ä»åŸ recorder åŠ è½½ model.pkl
    3. ç”¨ inject_config() æ„å»ºæ–°çš„ datasetï¼ˆæ–°çš„æ—¥æœŸèŒƒå›´ï¼‰
    4. model.predict(dataset)
    5. åœ¨æ–°å®éªŒä¸‹åˆ›å»º Recorderï¼Œä¿å­˜ pred.pkl å’Œ SignalRecord
    6. è®¡ç®— IC ç­‰æŒ‡æ ‡

    Args:
        model_name: æ¨¡å‹åç§°
        model_info: æ¨¡å‹æ³¨å†Œè¡¨ä¿¡æ¯ï¼ˆå« yaml_file ç­‰ï¼‰
        params: æ—¥æœŸå‚æ•°ï¼ˆæ¥è‡ª calculate_datesï¼‰
        experiment_name: æ–° MLflow å®éªŒåç§°
        source_records: æºè®­ç»ƒè®°å½• dict

    Returns:
        dict: {
            'success': bool,
            'record_id': str or None,
            'performance': dict or None,
            'error': str or None
        }
    """
    from train_utils import inject_config, PREDICTION_OUTPUT_DIR

    result = {
        'success': False,
        'record_id': None,
        'performance': None,
        'error': None
    }

    yaml_file = model_info['yaml_file']
    if not os.path.exists(yaml_file):
        result['error'] = f"YAML é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_file}"
        print(f"!!! Warning: {yaml_file} not found, skipping...")
        return result

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨äºæºè®°å½•ä¸­
    source_models = source_records.get('models', {})
    if model_name not in source_models:
        result['error'] = f"æ¨¡å‹ '{model_name}' ä¸åœ¨æºè®­ç»ƒè®°å½•ä¸­ï¼Œæ— æ³•åŠ è½½å·²æœ‰æ¨¡å‹"
        print(f"!!! Error: {result['error']}")
        return result

    source_record_id = source_models[model_name]
    source_experiment = source_records.get('experiment_name', 'Weekly_Production_Train')

    from qlib.utils import init_instance_by_config
    from qlib.workflow import R

    print(f"\n>>> Predict-Only: {model_name}")
    print(f"    Source: experiment={source_experiment}, recorder={source_record_id}")
    print(f"    YAML: {yaml_file}")

    try:
        # 1. ä»æº recorder åŠ è½½æ¨¡å‹
        print(f"[{model_name}] Loading model from source recorder...")
        source_recorder = R.get_recorder(
            recorder_id=source_record_id,
            experiment_name=source_experiment
        )
        model = source_recorder.load_object("model.pkl")
        print(f"[{model_name}] Model loaded successfully")

        # 2. æ„å»ºæ–°çš„ datasetï¼ˆä½¿ç”¨æ–°æ—¥æœŸèŒƒå›´ï¼‰
        task_config = inject_config(yaml_file, params)

        dataset_cfg = task_config['task']['dataset']
        dataset = init_instance_by_config(dataset_cfg)

        # 3. åœ¨æ–°å®éªŒä¸‹åˆ›å»º Recorder å¹¶é¢„æµ‹
        with R.start(experiment_name=experiment_name):
            R.set_tags(
                model=model_name,
                anchor_date=params['anchor_date'],
                mode='predict_only',
                source_experiment=source_experiment,
                source_record_id=source_record_id,
            )
            R.log_params(**params)

            # é¢„æµ‹
            print(f"[{model_name}] Predicting...")
            pred = model.predict(dataset=dataset)

            # ä¿å­˜é¢„æµ‹ç»“æœä¸º CSVï¼ˆä¾¿äºäººå·¥æŸ¥çœ‹ï¼‰
            os.makedirs(PREDICTION_OUTPUT_DIR, exist_ok=True)
            pred_file = os.path.join(
                PREDICTION_OUTPUT_DIR,
                f"{model_name}_{params['anchor_date']}.csv"
            )
            pred.to_csv(pred_file)
            print(f"[{model_name}] Predictions saved to {pred_file}")

            # è¿è¡Œ SignalRecordï¼ˆç”Ÿæˆ pred.pkl + sig_analysis/ic.pklï¼‰
            record_cfgs = task_config['task'].get('record', [])
            recorder = R.get_recorder()

            for r_cfg in record_cfgs:
                if r_cfg['kwargs'].get('model') == '<MODEL>':
                    r_cfg['kwargs']['model'] = model
                if r_cfg['kwargs'].get('dataset') == '<DATASET>':
                    r_cfg['kwargs']['dataset'] = dataset

                r_obj = init_instance_by_config(r_cfg, recorder=recorder)
                r_obj.generate()

            # è·å– IC æŒ‡æ ‡
            performance = {}
            try:
                ic_series = recorder.load_object("sig_analysis/ic.pkl")
                ic_mean = ic_series.mean()
                ic_std = ic_series.std()
                ic_ir = ic_mean / ic_std if ic_std != 0 else None
                performance = {
                    "IC_Mean": float(ic_mean) if ic_mean else None,
                    "ICIR": float(ic_ir) if ic_ir else None,
                    "record_id": recorder.info['id'],
                }
            except Exception as e:
                print(f"[{model_name}] Could not get IC metrics: {e}")
                performance = {"record_id": recorder.info['id']}

            rid = recorder.info['id']
            print(f"[{model_name}] Finished! New Recorder ID: {rid}")

            result['success'] = True
            result['record_id'] = rid
            result['performance'] = performance

    except Exception as e:
        result['error'] = str(e)
        print(f"!!! Error running predict-only for {model_name}: {e}")
        import traceback
        traceback.print_exc()

    return result


def run_predict_only(args):
    """æ‰§è¡Œ predict-only æµç¨‹"""
    from train_utils import (
        calculate_dates,
        merge_train_records,
        merge_performance_file,
        print_model_table,
        PREDICTION_OUTPUT_DIR,
        RECORD_OUTPUT_FILE,
    )

    # åŠ è½½æºè®­ç»ƒè®°å½•
    source_file = args.source_records
    if not os.path.exists(source_file):
        print(f"âŒ æºè®­ç»ƒè®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
        print("   è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆ latest_train_records.json")
        return

    with open(source_file, 'r') as f:
        source_records = json.load(f)

    print(f"ğŸ“‚ æºè®­ç»ƒè®°å½•: {source_file}")
    print(f"   å®éªŒ: {source_records.get('experiment_name', 'N/A')}")
    print(f"   é”šç‚¹æ—¥æœŸ: {source_records.get('anchor_date', 'N/A')}")
    print(f"   æ¨¡å‹æ•°: {len(source_records.get('models', {}))}")

    # è§£æç›®æ ‡æ¨¡å‹
    targets = resolve_target_models(args)
    if targets is None:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šè‡³å°‘ä¸€ç§æ¨¡å‹é€‰æ‹©æ–¹å¼")
        print("   ä½¿ç”¨ --models, --algorithm, --dataset, --tag, æˆ– --all-enabled")
        print("   ä½¿ç”¨ --help æŸ¥çœ‹å®Œæ•´å¸®åŠ©")
        return

    if not targets:
        print("âš ï¸  æ²¡æœ‰åŒ¹é…çš„æ¨¡å‹")
        return

    # æ£€æŸ¥å“ªäº›æ¨¡å‹åœ¨æºè®°å½•ä¸­å­˜åœ¨
    source_models = source_records.get('models', {})
    available = {k: v for k, v in targets.items() if k in source_models}
    missing = {k: v for k, v in targets.items() if k not in source_models}

    if missing:
        print(f"\nâš ï¸  ä»¥ä¸‹æ¨¡å‹ä¸åœ¨æºè®­ç»ƒè®°å½•ä¸­ï¼Œå°†è·³è¿‡:")
        for name in missing:
            print(f"    - {name}")

    if not available:
        print("âŒ æ²¡æœ‰å¯é¢„æµ‹çš„æ¨¡å‹ï¼ˆæ‰€æœ‰é€‰å®šæ¨¡å‹éƒ½ä¸åœ¨æºè®­ç»ƒè®°å½•ä¸­ï¼‰")
        return

    print_model_table(available, title="å¾…é¢„æµ‹æ¨¡å‹")

    # Dry-run æ¨¡å¼
    if args.dry_run:
        print("ğŸ” Dry-run æ¨¡å¼: ä»¥ä¸Šæ¨¡å‹å°†è¢«é¢„æµ‹ï¼Œä½†æœ¬æ¬¡ä¸ä¼šå®é™…æ‰§è¡Œ")
        print("   å»æ‰ --dry-run å‚æ•°ä»¥å®é™…è¿è¡Œ")
        return

    # ===== å¼€å§‹é¢„æµ‹ =====
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹ Predict-Only")
    print("=" * 60)

    # åˆå§‹åŒ– Qlib
    init_qlib()

    # è®¡ç®—æ—¥æœŸ
    params = calculate_dates()
    freq = params.get('freq', 'week').upper()

    experiment_name = args.experiment_name
    if experiment_name == DEFAULT_EXPERIMENT_NAME:
        experiment_name = f"{DEFAULT_EXPERIMENT_NAME}_{freq}"

    # æ”¶é›†ç»“æœ
    new_records = {
        "experiment_name": experiment_name,
        "anchor_date": params['anchor_date'],
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "models": {}
    }
    new_performances = {}
    failed_models = {}

    total = len(available)
    for idx, (model_name, model_info) in enumerate(available.items(), 1):
        print(f"\n{'â”€' * 60}")
        print(f"  [{idx}/{total}] {model_name}")
        print(f"{'â”€' * 60}")

        result = predict_single_model(
            model_name, model_info, params,
            experiment_name, source_records
        )

        if result['success']:
            new_records['models'][model_name] = result['record_id']
            if result['performance']:
                new_performances[model_name] = result['performance']
        else:
            failed_models[model_name] = result.get('error', 'Unknown error')
            print(f"âŒ æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {result.get('error', 'Unknown')}")

    # ===== åˆå¹¶è®°å½• =====
    if new_records['models']:
        print("\n" + "=" * 60)
        print("ğŸ“¦ åˆå¹¶é¢„æµ‹è®°å½•")
        print("=" * 60)

        # åˆå¹¶ latest_train_records.jsonï¼ˆmerge è¯­ä¹‰ï¼‰
        merged = merge_train_records(new_records)

        # åˆå¹¶æ€§èƒ½æ–‡ä»¶
        if new_performances:
            merge_performance_file(new_performances, params['anchor_date'])

    # ===== é¢„æµ‹æ€»ç»“ =====
    print(f"\n{'=' * 60}")
    print("ğŸ“Š Predict-Only å®Œæˆ")
    print("=" * 60)

    succeeded = [m for m in new_records['models']]
    print(f"  âœ… æˆåŠŸ: {len(succeeded)} ä¸ªæ¨¡å‹")
    for name in succeeded:
        perf = new_performances.get(name, {})
        ic = perf.get('IC_Mean', 'N/A')
        icir = perf.get('ICIR', 'N/A')
        ic_str = f"{ic:.4f}" if isinstance(ic, float) else ic
        icir_str = f"{icir:.4f}" if isinstance(icir, float) else icir
        print(f"    {name}: IC={ic_str}, ICIR={icir_str}")

    if failed_models:
        print(f"  âŒ å¤±è´¥: {len(failed_models)} ä¸ªæ¨¡å‹")
        for name, err in failed_models.items():
            print(f"    {name}: {err[:80]}")

    if missing:
        print(f"  â­ï¸  è·³è¿‡ï¼ˆä¸åœ¨æºè®°å½•ä¸­ï¼‰: {len(missing)} ä¸ªæ¨¡å‹")
        for name in missing:
            print(f"    {name}")

    print(f"\n  ğŸ“‚ å®éªŒå: {experiment_name}")
    print(f"  ğŸ“‹ è®°å½•å·²åˆå¹¶åˆ°: latest_train_records.json")
    print(f"  ğŸ“Š é¢„æµ‹ CSV åœ¨: output/predictions/")
    print(f"\n  ğŸ’¡ åç»­æ­¥éª¤:")
    print(f"     ç©·ä¸¾: python engine/scripts/brute_force_fast.py --max-combo-size 3")
    print(f"     èåˆ: python engine/scripts/ensemble_fusion.py --models <æ¨¡å‹åˆ—è¡¨>")
    print(f"{'=' * 60}\n")


def show_list(args):
    """åˆ—å‡ºæ¨¡å‹æ³¨å†Œè¡¨"""
    from train_utils import (
        load_model_registry,
        get_models_by_filter,
        print_model_table,
    )

    registry = load_model_registry()

    # åº”ç”¨ç­›é€‰æ¡ä»¶
    if args.algorithm or args.dataset or args.market or args.tag:
        models = get_models_by_filter(
            registry,
            algorithm=args.algorithm,
            dataset=args.dataset,
            market=args.market,
            tag=args.tag
        )
        title = "ç­›é€‰ç»“æœ"
    else:
        models = registry
        title = "å…¨éƒ¨æ³¨å†Œæ¨¡å‹"

    print_model_table(models, title=title)

    # æ‰“å°å¯ç”¨/ç¦ç”¨ç»Ÿè®¡
    enabled_count = sum(1 for m in models.values() if m.get('enabled', False))
    disabled_count = len(models) - enabled_count
    print(f"  å¯ç”¨: {enabled_count}  |  ç¦ç”¨: {disabled_count}")

    # æ£€æŸ¥æºè®­ç»ƒè®°å½•ä¸­å“ªäº›æ¨¡å‹å¯ç”¨
    source_file = args.source_records
    if os.path.exists(source_file):
        with open(source_file, 'r') as f:
            source_records = json.load(f)
        source_models = source_records.get('models', {})
        available = [name for name in models if name in source_models]
        print(f"\n  æºè®°å½• ({source_file}):")
        print(f"    å·²è®­ç»ƒå¯é¢„æµ‹: {len(available)} / {len(models)}")
        if available:
            print(f"    å¯ç”¨: {', '.join(available)}")
        not_available = [name for name in models if name not in source_models]
        if not_available:
            print(f"    æ— è®°å½•: {', '.join(not_available)}")
    else:
        print(f"\n  âš ï¸  æºè®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")


def main():
    import env
    env.safeguard("Prod Predict Only")
    args = parse_args()

    # ä¿¡æ¯æŸ¥çœ‹ç±»å‘½ä»¤ï¼ˆä¸éœ€è¦ Qlib åˆå§‹åŒ–ï¼‰
    if args.list:
        show_list(args)
        return

    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ¨¡å‹
    has_selection = any([
        args.models, args.algorithm, args.dataset,
        args.market, args.tag, args.all_enabled
    ])

    if not has_selection:
        print("âŒ è¯·æŒ‡å®šè¦é¢„æµ‹çš„æ¨¡å‹")
        print("   ä½¿ç”¨ --help æŸ¥çœ‹å®Œæ•´å¸®åŠ©")
        print("   ä½¿ç”¨ --list æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
        return

    run_predict_only(args)


if __name__ == "__main__":
    main()
