#!/usr/bin/env python
"""
è®­ç»ƒå…±äº«å·¥å…·æ¨¡å— (Train Utilities)
ä» weekly_train_predict.py æå–çš„å…±äº«é€»è¾‘ï¼Œä¾›å…¨é‡è®­ç»ƒå’Œå¢é‡è®­ç»ƒå¤ç”¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ—¥æœŸè®¡ç®— (calculate_dates)
- YAML å‚æ•°æ³¨å…¥ (inject_config)
- æ¨¡å‹æ³¨å†Œè¡¨ç®¡ç† (load_model_registry, get_models_by_filter)
- å•æ¨¡å‹è®­ç»ƒæµç¨‹ (train_single_model)
- å†å²å¤‡ä»½ (backup_file_with_date)
- è®­ç»ƒè®°å½•åˆå¹¶ (merge_train_records)
- è¿è¡ŒçŠ¶æ€ç®¡ç† (save_run_state, load_run_state)
"""

import os
import json
import yaml
import shutil
from datetime import datetime, timedelta

# æ³¨æ„: qlib ç›¸å…³å¯¼å…¥ï¼ˆD, init_instance_by_config, Rï¼‰åœ¨éœ€è¦çš„å‡½æ•°å†…éƒ¨å»¶è¿Ÿå¯¼å…¥ï¼Œ
# è¿™æ · --list, --show-state ç­‰ä¸éœ€è¦è®­ç»ƒçš„å‘½ä»¤å¯ä»¥åœ¨æ²¡æœ‰ qlib çš„ç¯å¢ƒä¸­è¿è¡Œã€‚


# ================= è·¯å¾„å¸¸é‡ =================
import gc
import traceback

import env

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = env.ROOT_DIR

REGISTRY_FILE = os.path.join(ROOT_DIR, "config", "model_registry.yaml")
MODEL_CONFIG_FILE = os.path.join(ROOT_DIR, "config", "model_config.json")
WEEKLY_CONFIG_FILE = os.path.join(ROOT_DIR, "config", "weekly_config.json")
RECORD_OUTPUT_FILE = os.path.join(ROOT_DIR, "latest_train_records.json")
PREDICTION_OUTPUT_DIR = os.path.join(ROOT_DIR, "output", "predictions")
HISTORY_DIR = os.path.join(ROOT_DIR, "data", "history")
RUN_STATE_FILE = os.path.join(ROOT_DIR, "data", "run_state.json")


# ================= æ—¥æœŸè®¡ç®— =================
def calculate_dates():
    """æ ¹æ® model_config.json è®¡ç®—è®­ç»ƒæ—¥æœŸçª—å£"""
    from qlib.data import D

    with open(MODEL_CONFIG_FILE, 'r') as file:
        config = json.load(file)

    train_date_mode = config['train_date_mode']
    data_slice_mode = config['data_slice_mode']

    test_set_window = config["test_set_window"]
    valid_set_window = config["valid_set_window"]
    train_set_windows = config["train_set_windows"]

    # ç¡®å®šé”šç‚¹æ—¥æœŸ
    if train_date_mode == 'last_trade_date':
        last_trade_date = D.calendar(future=False)[-1:][0]
        anchor_date = last_trade_date.strftime('%Y-%m-%d')
    else:
        anchor_date = config.get('current_date', datetime.now().strftime('%Y-%m-%d'))

    def add_year_with_nextday(input_date, n):
        input_date_obj = datetime.strptime(input_date, "%Y-%m-%d")
        added_year_date = datetime(input_date_obj.year + n, input_date_obj.month, input_date_obj.day)
        next_day = added_year_date + timedelta(days=1)
        return added_year_date.strftime("%Y-%m-%d"), next_day.strftime("%Y-%m-%d")

    if data_slice_mode == 'slide':
        _, start_time = add_year_with_nextday(anchor_date, -1 * (train_set_windows + valid_set_window + test_set_window))
        fit_start_time = start_time
        fit_end_time, valid_start_time = add_year_with_nextday(anchor_date, -1 * (valid_set_window + test_set_window))
        valid_end_time, test_start_time = add_year_with_nextday(anchor_date, -1 * test_set_window)
        test_end_time = anchor_date
    else:
        start_time = config.get("start_time", "2008-01-01")
        fit_start_time = config.get("fit_start_time", "2008-01-01")
        fit_end_time = config["fit_end_time"]
        valid_start_time = config["valid_start_time"]
        valid_end_time = config["valid_end_time"]
        test_start_time = config["test_start_time"]
        test_end_time = config["test_end_time"]

    # åŠ è½½å‘¨é…ç½®æ–‡ä»¶ä»¥è·å–å½“å‰èµ„é‡‘ä¿¡æ¯
    with open(WEEKLY_CONFIG_FILE, 'r') as file:
        weekly_config = json.load(file)
    
    account = weekly_config.get("current_full_cash", 100000.0)

    date_params = {
        "market": config["market"],
        "benchmark": config["benchmark"],
        "topk": config["TopK"],
        "n_drop": config["DropN"],
        "account": account,
        "start_time": start_time,
        "end_time": test_end_time,
        "fit_start_time": fit_start_time,
        "fit_end_time": fit_end_time,
        "valid_start_time": valid_start_time,
        "valid_end_time": valid_end_time,
        "test_start_time": test_start_time,
        "test_end_time": test_end_time,
        "anchor_date": anchor_date
    }

    print("\n=== Date Calculation Result ===")
    for k, v in date_params.items():
        print(f"{k}: {v}")
    print("===============================\n")

    return date_params


# ================= YAML æ³¨å…¥ =================
def inject_config(yaml_path, params):
    """å°†æ—¥æœŸå‚æ•°æ³¨å…¥ YAML é…ç½®"""
    with open(yaml_path, 'r') as f:
        config = yaml.safe_load(f)

    config['market'] = params['market']
    config['benchmark'] = params['benchmark']

    dh = config['data_handler_config']
    dh['start_time'] = params['start_time']
    dh['end_time'] = params['end_time']
    dh['fit_start_time'] = params['fit_start_time']
    dh['fit_end_time'] = params['fit_end_time']
    dh['instruments'] = params['market']

    if 'task' in config and 'dataset' in config['task']:
        segs = config['task']['dataset']['kwargs']['segments']
        segs['train'] = [params['fit_start_time'], params['fit_end_time']]
        segs['valid'] = [params['valid_start_time'], params['valid_end_time']]
        segs['test'] = [params['test_start_time'], params['test_end_time']]

    if 'port_analysis_config' in config:
        pa = config['port_analysis_config']
        if 'strategy' in pa and 'kwargs' in pa['strategy']:
            pa['strategy']['kwargs']['topk'] = params['topk']
            pa['strategy']['kwargs']['n_drop'] = params['n_drop']
        if 'backtest' in pa:
            pa['backtest']['start_time'] = params['test_start_time']
            pa['backtest']['end_time'] = params['test_end_time']
            pa['backtest']['account'] = params['account']
            pa['backtest']['benchmark'] = params['benchmark']

    return config


# ================= æ¨¡å‹æ³¨å†Œè¡¨ =================
def load_model_registry(registry_file=None):
    """
    åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨
    
    Returns:
        dict: {model_name: {algorithm, dataset, market, yaml_file, enabled, tags, notes}}
    """
    if registry_file is None:
        registry_file = REGISTRY_FILE
    
    with open(registry_file, 'r') as f:
        registry = yaml.safe_load(f)
    
    return registry.get('models', {})


def get_enabled_models(registry=None):
    """è·å–æ‰€æœ‰ enabled=true çš„æ¨¡å‹åˆ—è¡¨"""
    if registry is None:
        registry = load_model_registry()
    
    return {name: info for name, info in registry.items() if info.get('enabled', False)}


def get_models_by_filter(registry=None, algorithm=None, dataset=None, market=None, tag=None):
    """
    æŒ‰æ¡ä»¶ç­›é€‰æ¨¡å‹
    
    Args:
        registry: æ¨¡å‹æ³¨å†Œè¡¨ï¼ŒNone åˆ™è‡ªåŠ¨åŠ è½½
        algorithm: æŒ‰ç®—æ³•ç­›é€‰ (å¦‚ 'lstm', 'gru')
        dataset: æŒ‰æ•°æ®é›†ç­›é€‰ (å¦‚ 'Alpha158', 'Alpha360')
        market: æŒ‰å¸‚åœºç­›é€‰ (å¦‚ 'csi300')
        tag: æŒ‰æ ‡ç­¾ç­›é€‰ (å¦‚ 'ts', 'tree')
    
    Returns:
        dict: æ»¡è¶³æ¡ä»¶çš„æ¨¡å‹å­é›†
    """
    if registry is None:
        registry = load_model_registry()
    
    result = {}
    for name, info in registry.items():
        if algorithm and info.get('algorithm', '').lower() != algorithm.lower():
            continue
        if dataset and info.get('dataset', '').lower() != dataset.lower():
            continue
        if market and info.get('market', '').lower() != market.lower():
            continue
        if tag and tag.lower() not in [t.lower() for t in info.get('tags', [])]:
            continue
        result[name] = info
    
    return result


def get_models_by_names(model_names, registry=None):
    """
    æŒ‰æ¨¡å‹ååˆ—è¡¨è·å–æ¨¡å‹ä¿¡æ¯
    
    Args:
        model_names: æ¨¡å‹ååˆ—è¡¨
        registry: æ¨¡å‹æ³¨å†Œè¡¨ï¼ŒNone åˆ™è‡ªåŠ¨åŠ è½½
    
    Returns:
        dict: åŒ¹é…çš„æ¨¡å‹å­é›†
    """
    if registry is None:
        registry = load_model_registry()
    
    result = {}
    for name in model_names:
        name = name.strip()
        if name in registry:
            result[name] = registry[name]
        else:
            print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{name}' ä¸åœ¨æ³¨å†Œè¡¨ä¸­ï¼Œè·³è¿‡")
    
    return result


# ================= å•æ¨¡å‹è®­ç»ƒ =================
def train_single_model(model_name, yaml_file, params, experiment_name):
    """
    è®­ç»ƒå•ä¸ªæ¨¡å‹çš„å®Œæ•´æµç¨‹ï¼šè®­ç»ƒ â†’ é¢„æµ‹ â†’ Signal Record â†’ IC è®¡ç®—
    
    éœ€è¦ qlib å·²åˆå§‹åŒ–ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°
        yaml_file: YAML é…ç½®æ–‡ä»¶è·¯å¾„
        params: æ—¥æœŸå‚æ•°ï¼ˆæ¥è‡ª calculate_datesï¼‰
        experiment_name: MLflow å®éªŒåç§°
    
    Returns:
        dict: {
            'success': bool,
            'record_id': str or None,
            'performance': dict or None,
            'error': str or None
        }
    """
    result = {
        'success': False,
        'record_id': None,
        'performance': None,
        'error': None
    }
    
    if not os.path.exists(yaml_file):
        result['error'] = f"YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {yaml_file}"
        print(f"!!! Warning: {yaml_file} not found, skipping...")
        return result
    
    from qlib.utils import init_instance_by_config
    from qlib.workflow import R

    print(f"\n>>> Processing Model: {model_name} from {yaml_file} ...")
    
    task_config = inject_config(yaml_file, params)
    
    try:
        with R.start(experiment_name=experiment_name):
            R.set_tags(model=model_name, anchor_date=params['anchor_date'])
            R.log_params(**params)
            
            # åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®é›†
            model_cfg = task_config['task']['model']
            model = init_instance_by_config(model_cfg)
            
            dataset_cfg = task_config['task']['dataset']
            dataset = init_instance_by_config(dataset_cfg)
            
            # è®­ç»ƒ
            print(f"[{model_name}] Training...")
            if 'lstm' in model_name.lower():
                model.fit(dataset=dataset, save_path="csi300_lstm_ts_latest.pkl")
            else:
                model.fit(dataset=dataset)
            
            # é¢„æµ‹
            print(f"[{model_name}] Predicting...")
            pred = model.predict(dataset=dataset)
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            os.makedirs(PREDICTION_OUTPUT_DIR, exist_ok=True)
            pred_file = os.path.join(PREDICTION_OUTPUT_DIR, f"{model_name}_{params['anchor_date']}.csv")
            pred.to_csv(pred_file)
            print(f"[{model_name}] Predictions saved to {pred_file}")
            
            # ç”Ÿæˆ Signal Record
            record_cfgs = task_config['task'].get('record', [])
            recorder = R.get_recorder()
            
            for r_cfg in record_cfgs:
                if r_cfg['kwargs'].get('model') == '<MODEL>':
                    r_cfg['kwargs']['model'] = model
                if r_cfg['kwargs'].get('dataset') == '<DATASET>':
                    r_cfg['kwargs']['dataset'] = dataset
                
                r_obj = init_instance_by_config(r_cfg, recorder=recorder)
                r_obj.generate()
            
            # è·å–æ¨¡å‹æˆç»©ï¼ˆICç­‰ï¼‰
            performance = {}
            try:
                ic_series = recorder.load_object("sig_analysis/ic.pkl")
                ic_mean = ic_series.mean()
                ic_std = ic_series.std()
                ic_ir = ic_mean / ic_std if ic_std != 0 else None
                performance = {
                    "IC_Mean": float(ic_mean) if ic_mean else None,
                    "ICIR": float(ic_ir) if ic_ir else None,
                    "record_id": recorder.info['id']
                }
            except Exception as e:
                print(f"[{model_name}] Could not get IC metrics: {e}")
                performance = {"record_id": recorder.info['id']}
            
            rid = recorder.info['id']
            print(f"[{model_name}] Finished! Recorder ID: {rid}")
            
            result['success'] = True
            result['record_id'] = rid
            result['performance'] = performance
    
    except Exception as e:
        result['error'] = str(e)
        print(f"!!! Error running {model_name}: {e}")
        import traceback
        traceback.print_exc()
    
    return result


# ================= å†å²å¤‡ä»½ =================
def backup_file_with_date(file_path, history_dir=None, prefix=None):
    """
    å°†æ–‡ä»¶å¤‡ä»½åˆ° history ç›®å½•ï¼Œæ–‡ä»¶åå¸¦æ—¥æœŸæ—¶é—´æˆ³
    
    Args:
        file_path: è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„
        history_dir: å†å²ç›®å½•ï¼Œé»˜è®¤ data/history/
        prefix: å¤‡ä»½æ–‡ä»¶åå‰ç¼€ï¼Œé»˜è®¤ä½¿ç”¨åŸæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    
    Returns:
        str: å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæºæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å› None
    """
    if not os.path.exists(file_path):
        return None
    
    if history_dir is None:
        history_dir = HISTORY_DIR
    
    os.makedirs(history_dir, exist_ok=True)
    
    basename = os.path.basename(file_path)
    name, ext = os.path.splitext(basename)
    if prefix:
        name = prefix
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    backup_name = f"{name}_{timestamp}{ext}"
    backup_path = os.path.join(history_dir, backup_name)
    
    shutil.copy2(file_path, backup_path)
    print(f"ğŸ“¦ å·²å¤‡ä»½: {file_path} â†’ {backup_path}")
    
    return backup_path


# ================= è®­ç»ƒè®°å½•ç®¡ç† =================
def merge_train_records(new_records, record_file=None):
    """
    å¢é‡åˆå¹¶è®­ç»ƒè®°å½•åˆ° latest_train_records.json
    
    è¯­ä¹‰ï¼š
    - åŒåæ¨¡å‹ â†’ è¦†ç›– recorder ID
    - æ–°æ¨¡å‹ â†’ è¿½åŠ 
    - æœªå‡ºç°çš„æ¨¡å‹ â†’ ä¿ç•™åŸæœ‰è®°å½•
    
    Args:
        new_records: æ–°çš„è®­ç»ƒè®°å½• dictï¼Œæ ¼å¼åŒ latest_train_records.json
        record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åˆå¹¶åçš„å®Œæ•´è®°å½•
    """
    if record_file is None:
        record_file = RECORD_OUTPUT_FILE
    
    # å…ˆå¤‡ä»½ç°æœ‰æ–‡ä»¶
    backup_file_with_date(record_file, prefix="train_records")
    
    # åŠ è½½ç°æœ‰è®°å½•
    existing = {}
    if os.path.exists(record_file):
        with open(record_file, 'r') as f:
            existing = json.load(f)
    
    # åˆå¹¶ï¼šä¿ç•™æ—¢æœ‰æ¨¡å‹ï¼Œè¦†ç›–/æ–°å¢è®­ç»ƒè¿‡çš„æ¨¡å‹
    merged_models = existing.get('models', {}).copy()
    new_models = new_records.get('models', {})
    
    added = []
    updated = []
    for model_name, rid in new_models.items():
        if model_name in merged_models:
            if merged_models[model_name] != rid:
                updated.append(model_name)
        else:
            added.append(model_name)
        merged_models[model_name] = rid
    
    # æ„å»ºåˆå¹¶åçš„è®°å½•
    merged = {
        "experiment_name": new_records.get('experiment_name', existing.get('experiment_name', '')),
        "anchor_date": new_records.get('anchor_date', existing.get('anchor_date', '')),
        "timestamp": new_records.get('timestamp', datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
        "last_incremental_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "models": merged_models
    }
    
    # ä¿å­˜
    with open(record_file, 'w') as f:
        json.dump(merged, f, indent=4)
    
    # æ‰“å°åˆå¹¶æ‘˜è¦
    preserved = [m for m in merged_models if m not in new_models]
    print(f"\nğŸ“‹ è®­ç»ƒè®°å½•åˆå¹¶å®Œæˆ:")
    if updated:
        print(f"  ğŸ”„ æ›´æ–° ({len(updated)}): {', '.join(updated)}")
    if added:
        print(f"  â• æ–°å¢ ({len(added)}): {', '.join(added)}")
    if preserved:
        print(f"  ğŸ“Œ ä¿ç•™ ({len(preserved)}): {', '.join(preserved)}")
    print(f"  ğŸ“ æ–‡ä»¶: {record_file}")
    
    return merged


def overwrite_train_records(records, record_file=None):
    """
    å…¨é‡è¦†å†™è®­ç»ƒè®°å½•ï¼ˆç”¨äº weekly_train_predict.py å…¨é‡åˆ·æ–°æ¨¡å¼ï¼‰
    è¦†å†™å‰è‡ªåŠ¨å¤‡ä»½ã€‚
    
    Args:
        records: å®Œæ•´çš„è®­ç»ƒè®°å½• dict
        record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    """
    if record_file is None:
        record_file = RECORD_OUTPUT_FILE
    
    # å¤‡ä»½ç°æœ‰æ–‡ä»¶
    backup_file_with_date(record_file, prefix="train_records")
    
    # å…¨é‡è¦†å†™
    with open(record_file, 'w') as f:
        json.dump(records, f, indent=4)
    
    print(f"ğŸ“‹ è®­ç»ƒè®°å½•å·²å…¨é‡è¦†å†™: {record_file}")


def merge_performance_file(new_performances, anchor_date, output_dir=None):
    """
    åˆå¹¶æ¨¡å‹æ€§èƒ½æ–‡ä»¶
    
    Args:
        new_performances: æ–°çš„æ€§èƒ½æ•°æ® dict
        anchor_date: é”šç‚¹æ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        dict: åˆå¹¶åçš„æ€§èƒ½æ•°æ®
    """
    if output_dir is None:
        output_dir = os.path.join(ROOT_DIR, "output")
    
    perf_file = os.path.join(output_dir, f"model_performance_{anchor_date}.json")
    
    # åŠ è½½ç°æœ‰æ€§èƒ½æ•°æ®
    existing = {}
    if os.path.exists(perf_file):
        backup_file_with_date(perf_file, prefix=f"model_performance_{anchor_date}")
        with open(perf_file, 'r') as f:
            existing = json.load(f)
    
    # åˆå¹¶
    merged = existing.copy()
    merged.update(new_performances)
    
    # ä¿å­˜
    with open(perf_file, 'w') as f:
        json.dump(merged, f, indent=4)
    
    return merged


# ================= è¿è¡ŒçŠ¶æ€ç®¡ç† =================
def save_run_state(state, state_file=None):
    """
    ä¿å­˜è¿è¡ŒçŠ¶æ€ï¼ˆç”¨äº rerun/resumeï¼‰
    
    Args:
        state: è¿è¡ŒçŠ¶æ€ dictï¼Œæ ¼å¼:
            {
                'started_at': str,
                'mode': 'incremental' | 'full',
                'target_models': [str],
                'completed': [str],
                'failed': {model_name: error_msg},
                'skipped': [str]
            }
    """
    if state_file is None:
        state_file = RUN_STATE_FILE
    
    os.makedirs(os.path.dirname(state_file), exist_ok=True)
    
    with open(state_file, 'w') as f:
        json.dump(state, f, indent=4)


def load_run_state(state_file=None):
    """
    åŠ è½½è¿è¡ŒçŠ¶æ€
    
    Returns:
        dict or None: è¿è¡ŒçŠ¶æ€ï¼Œä¸å­˜åœ¨åˆ™è¿”å› None
    """
    if state_file is None:
        state_file = RUN_STATE_FILE
    
    if os.path.exists(state_file):
        with open(state_file, 'r') as f:
            return json.load(f)
    return None


def clear_run_state(state_file=None):
    """æ¸…é™¤è¿è¡ŒçŠ¶æ€æ–‡ä»¶"""
    if state_file is None:
        state_file = RUN_STATE_FILE
    
    if os.path.exists(state_file):
        # å¤‡ä»½åˆ°å†å²
        backup_file_with_date(state_file, prefix="run_state")
        os.remove(state_file)
        print("ğŸ—‘ï¸  è¿è¡ŒçŠ¶æ€å·²æ¸…é™¤")


# ================= å·¥å…·å‡½æ•° =================
def print_model_table(models, title="æ¨¡å‹åˆ—è¡¨"):
    """
    ä»¥è¡¨æ ¼å½¢å¼æ‰“å°æ¨¡å‹åˆ—è¡¨
    
    Args:
        models: æ¨¡å‹å­—å…¸ {name: info}
        title: è¡¨æ ¼æ ‡é¢˜
    """
    print(f"\n{'='*70}")
    print(f"  {title} ({len(models)} ä¸ªæ¨¡å‹)")
    print(f"{'='*70}")
    print(f"  {'æ¨¡å‹å':<30} {'ç®—æ³•':<12} {'æ•°æ®é›†':<12} {'å¸‚åœº':<8} {'æ ‡ç­¾'}")
    print(f"  {'-'*30} {'-'*12} {'-'*12} {'-'*8} {'-'*20}")
    
    for name, info in models.items():
        tags_str = ', '.join(info.get('tags', []))
        print(f"  {name:<30} {info.get('algorithm',''):<12} {info.get('dataset',''):<12} {info.get('market',''):<8} {tags_str}")
    
    print(f"{'='*70}\n")
