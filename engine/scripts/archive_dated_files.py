#!/usr/bin/env python3
"""
QuantPits å¸¦æ—¥æœŸæ–‡ä»¶å½’æ¡£å·¥å…·

å¸¸æ€åŒ–å½’æ¡£å·¥å…·ï¼Œæ¯å‘¨è¿è¡Œåæ¸…ç†å†å²æ–‡ä»¶ï¼š
- è‡ªåŠ¨æ‰«æ output/ åŠå­ç›®å½•ä¸­ *_YYYY-MM-DD* å’Œ YYYY-MM-DD-* æ ¼å¼çš„æ–‡ä»¶
- æŒ‰é€»è¾‘æ–‡ä»¶ååˆ†ç»„ï¼Œä¿ç•™æœ€æ–° N ä¸ªæ—¥æœŸç‰ˆæœ¬ï¼Œæ—§ç‰ˆç§»å…¥ archive/
- è®¢å•å»ºè®®å’Œäº¤æ˜“æ˜ç»†å½’æ¡£åˆ° data/order_history/
- æ”¯æŒ --dry-run æ¨¡å¼é¢„è§ˆ

ç”¨æ³•:
    python engine/scripts/archive_dated_files.py --dry-run          # é¢„è§ˆæ¨¡å¼
    python engine/scripts/archive_dated_files.py                     # å®é™…å½’æ¡£
    python engine/scripts/archive_dated_files.py --keep 2            # ä¿ç•™æœ€è¿‘2ä¸ªç‰ˆæœ¬
    python engine/scripts/archive_dated_files.py --include-notebooks # åŒæ—¶å½’æ¡£ legacy notebooks
    python engine/scripts/archive_dated_files.py --cleanup-legacy    # æ¸…ç†æµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶
"""

import argparse
import json
import os
import re
import shutil
import sys
from collections import defaultdict
from datetime import datetime

import env

# â”€â”€ è·¯å¾„å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT_DIR = env.ROOT_DIR
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
DATA_DIR = os.path.join(ROOT_DIR, "data")
ARCHIVE_DIR = os.path.join(ROOT_DIR, "archive")
ORDER_HISTORY_DIR = os.path.join(DATA_DIR, "order_history")

# â”€â”€ æ—¥æœŸæ¨¡å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŒ¹é… _YYYY-MM-DD æˆ– _YYYY-MM-DD_HHMMSSï¼ˆæ–‡ä»¶åä¸­é—´æˆ–æœ«å°¾ï¼‰
DATE_PATTERN_SUFFIX = re.compile(r'_(\d{4}-\d{2}-\d{2})(?:_(\d{6}))?')
# åŒ¹é… YYYY-MM-DD- å¼€å¤´çš„æ–‡ä»¶åï¼ˆå¦‚äº¤æ˜“è½¯ä»¶å¯¼å‡ºçš„ xlsxï¼‰
DATE_PATTERN_PREFIX = re.compile(r'^(\d{4}-\d{2}-\d{2})-')

# â”€â”€ äº¤æ˜“æ•°æ®æ–‡ä»¶æ¨¡å¼ï¼ˆå½’æ¡£åˆ° order_historyï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRADE_DATA_PATTERNS = [
    r'^buy_suggestion_',
    r'^sell_suggestion_',
    r'^model_opinions_',
    r'^trade_detail_',
    r'^\d{4}-\d{2}-\d{2}-table\.xlsx$',
]

# â”€â”€ Legacy notebooksï¼ˆå·²é‡æ„åˆ° scripts/ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEGACY_NOTEBOOKS = [
    "weekly_ensemble_predict.ipynb",
    "weekly_ensemble_predict_v2.ipynb",
    "weekly_ensemble_predict_v3.ipynb",
    "brute_force_data_check.ipynb",
    "weekly_order_gen.ipynb",
    "weekly_order_gen_ensemble.ipynb",
    "weekly_order_gen_v2.ipynb",
    "weekly_post_trade.ipynb",
]

# â”€â”€ æµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEGACY_ITEMS = {
    # ç›®å½•
    "output/Alpha158_full": "misc/Alpha158_full",
    "output/analysis": "misc/analysis",
    # å•æ–‡ä»¶
    "data/emp-table.xlsx": "misc/emp-table.xlsx",
    "scripts/compare_results.py": "misc/compare_results.py",
    "scripts/model_compare.py": "misc/model_compare.py",
    "scripts/ensemble_predict.py": "misc/ensemble_predict.py",
    # output/ æ ¹ç›®å½•çš„æ—©æœŸæµ‹è¯•æ•£è½æ–‡ä»¶ï¼ˆå·²è¢«å­ç›®å½•çš„æ­£å¼è¾“å‡ºå–ä»£ï¼‰
    "output/brute_force_results_2026-02-06.csv": "misc/brute_force_results_2026-02-06.csv",
    "output/best_model_2026-02-06.txt": "misc/best_model_2026-02-06.txt",
    "output/ensemble_config_2026-02-06.json": "misc/ensemble_config_2026-02-06.json",
    "output/ensemble_performance_2026-02-06.png": "misc/ensemble_performance_2026-02-06.png",
    "output/leaderboard_2026-02-06.csv": "misc/leaderboard_2026-02-06.csv",
    "output/model_comparison_2026-02-06.json": "misc/model_comparison_2026-02-06.json",
}

# â”€â”€ ä¸åº”è¢«å½’æ¡£çš„æ–‡ä»¶ï¼ˆç´¯è®¡æ—¥å¿—ç­‰ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROTECTED_PATTERNS = [
    r'_full\.csv$',       # trade_log_full.csv, holding_log_full.csv, etc.
    r'^run_state\.json$',
    r'^model_log\.csv$',
]


def extract_date_info(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸå’Œé€»è¾‘æ–‡ä»¶åï¼ˆå»æ‰æ—¥æœŸéƒ¨åˆ†çš„å‰ç¼€ï¼‰

    Returns:
        (logical_name, date_str, sort_key) or None
        - logical_name: å»æ‰æ—¥æœŸåçš„æ–‡ä»¶åå‰ç¼€ï¼ˆç”¨æ¥åˆ†ç»„ï¼‰
        - date_str: YYYY-MM-DD æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
        - sort_key: ç”¨äºæ’åºçš„å­—ç¬¦ä¸²ï¼ˆæ—¥æœŸ+å¯é€‰æ—¶é—´æˆ³ï¼‰
    """
    # å°è¯•å‰ç¼€æ¨¡å¼ï¼šYYYY-MM-DD-xxx
    m = DATE_PATTERN_PREFIX.match(filename)
    if m:
        date_str = m.group(1)
        # é€»è¾‘åï¼šæ—¥æœŸåé¢çš„éƒ¨åˆ†ï¼ˆå¦‚ "table.xlsx"ï¼‰
        rest = filename[len(m.group(0)):]
        name, ext = os.path.splitext(rest)
        logical_name = f"*-{rest}"  # é€šé…å‰ç¼€
        return logical_name, date_str, date_str

    # å°è¯•åç¼€æ¨¡å¼ï¼šxxx_YYYY-MM-DD æˆ– xxx_YYYY-MM-DD_HHMMSS
    m = DATE_PATTERN_SUFFIX.search(filename)
    if m:
        date_str = m.group(1)
        timestamp = m.group(2) or ""
        # é€»è¾‘åï¼šæ—¥æœŸä¹‹å‰çš„éƒ¨åˆ† + æ—¥æœŸä¹‹åçš„æ‰©å±•å
        prefix = filename[:m.start()]
        suffix = filename[m.end():]
        logical_name = f"{prefix}*{suffix}"
        sort_key = f"{date_str}_{timestamp}" if timestamp else date_str
        return logical_name, date_str, sort_key

    return None


def is_protected(filename):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å—ä¿æŠ¤ï¼ˆä¸åº”å½’æ¡£ï¼‰"""
    for pattern in PROTECTED_PATTERNS:
        if re.search(pattern, filename):
            return True
    return False


def is_trade_data(filename):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å±äºäº¤æ˜“æ•°æ®"""
    for pattern in TRADE_DATA_PATTERNS:
        if re.match(pattern, filename):
            return True
    return False


def scan_dated_files(directory, relative_prefix=""):
    """
    æ‰«æç›®å½•ä¸­çš„å¸¦æ—¥æœŸæ–‡ä»¶ï¼Œè¿”å›åˆ†ç»„ä¿¡æ¯

    Returns:
        dict: {(relative_dir, logical_name): [(sort_key, filename, full_path), ...]}
    """
    groups = defaultdict(list)

    if not os.path.isdir(directory):
        return groups

    for entry in os.listdir(directory):
        full_path = os.path.join(directory, entry)

        # è·³è¿‡ç›®å½•ï¼ˆå­ç›®å½•ç”±è°ƒç”¨è€…é€’å½’å¤„ç†ï¼‰
        if os.path.isdir(full_path):
            continue

        # è·³è¿‡å—ä¿æŠ¤çš„æ–‡ä»¶
        if is_protected(entry):
            continue

        info = extract_date_info(entry)
        if info:
            logical_name, date_str, sort_key = info
            rel_dir = relative_prefix
            groups[(rel_dir, logical_name)].append((sort_key, entry, full_path))

    return groups


def get_anchor_date(override=None):
    """
    è·å–é”šç‚¹æ—¥æœŸï¼ˆæœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé€šå¸¸æ˜¯å‘¨äº”ï¼‰

    ä¼˜å…ˆçº§ï¼š
    1. CLI ä¼ å…¥çš„ --anchor-date
    2. latest_train_records.json ä¸­çš„ anchor_date
    3. å¦‚æœéƒ½æ²¡æœ‰åˆ™æŠ¥é”™

    Returns:
        str: YYYY-MM-DD æ ¼å¼çš„é”šç‚¹æ—¥æœŸ
    """
    if override:
        return override

    records_file = os.path.join(ROOT_DIR, "latest_train_records.json")
    if os.path.exists(records_file):
        with open(records_file, 'r') as f:
            records = json.load(f)
        anchor = records.get('anchor_date', '')
        if anchor:
            return anchor

    raise ValueError(
        "æ— æ³•ç¡®å®šé”šç‚¹æ—¥æœŸã€‚è¯·é€šè¿‡ --anchor-date YYYY-MM-DD æŒ‡å®šï¼Œ"
        "æˆ–ç¡®ä¿ latest_train_records.json ä¸­åŒ…å« anchor_date å­—æ®µã€‚"
    )


def plan_archive(anchor_date):
    """
    è®¡ç®—éœ€è¦å½’æ¡£çš„æ–‡ä»¶ï¼šæ—¥æœŸ < anchor_date çš„æ–‡ä»¶å½’æ¡£ï¼Œ>= anchor_date çš„ä¿ç•™

    Args:
        anchor_date: é”šç‚¹æ—¥æœŸå­—ç¬¦ä¸² YYYY-MM-DD

    Returns:
        list of (source_path, dest_path, category)
    """
    moves = []

    # â”€â”€ 1. æ‰«æ output/ æ ¹ç›®å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    groups = scan_dated_files(OUTPUT_DIR, "output")

    # â”€â”€ 2. æ‰«æ output/ å­ç›®å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for subdir in ["predictions", "ensemble", "brute_force", "brute_force_fast", "ranking"]:
        sub_path = os.path.join(OUTPUT_DIR, subdir)
        sub_groups = scan_dated_files(sub_path, f"output/{subdir}")
        groups.update(sub_groups)

    # â”€â”€ 3. æ‰«æ data/ ç›®å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    data_groups = scan_dated_files(DATA_DIR, "data")
    groups.update(data_groups)

    # â”€â”€ 4. å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼šæ—¥æœŸ < anchor_date çš„å½’æ¡£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for (rel_dir, logical_name), files in sorted(groups.items()):
        for sort_key, filename, full_path in files:
            # sort_key æ ¼å¼ä¸º YYYY-MM-DD æˆ– YYYY-MM-DD_HHMMSS
            file_date = sort_key[:10]  # æå– YYYY-MM-DD éƒ¨åˆ†
            if file_date < anchor_date:
                # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šå½’æ¡£ç›®æ ‡
                if is_trade_data(filename):
                    dest = os.path.join(ORDER_HISTORY_DIR, filename)
                    category = "trade_data"
                else:
                    dest = os.path.join(ARCHIVE_DIR, rel_dir, filename)
                    category = "output"
                moves.append((full_path, dest, category))

    return moves


def archive_legacy_notebooks(dry_run=False):
    """å½’æ¡£å·²é‡æ„çš„ legacy notebooks"""
    moves = []
    notebooks_dir = os.path.join(ROOT_DIR, "notebooks")
    archive_nb_dir = os.path.join(ARCHIVE_DIR, "notebooks")

    for nb in LEGACY_NOTEBOOKS:
        src = os.path.join(notebooks_dir, nb)
        if os.path.exists(src):
            dest = os.path.join(archive_nb_dir, nb)
            moves.append((src, dest, "notebook"))

    # notebooks/output/ ç›®å½•ï¼ˆç©ºçš„ï¼‰
    nb_output = os.path.join(notebooks_dir, "output")
    if os.path.isdir(nb_output) and not os.listdir(nb_output):
        if not dry_run:
            os.rmdir(nb_output)
            print(f"  ğŸ—‘ï¸  åˆ é™¤ç©ºç›®å½•: notebooks/output/")
        else:
            print(f"  ğŸ—‘ï¸  [DRY-RUN] å°†åˆ é™¤ç©ºç›®å½•: notebooks/output/")

    return moves


def archive_legacy_items(dry_run=False):
    """å½’æ¡£æµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶"""
    moves = []

    for src_rel, dest_rel in LEGACY_ITEMS.items():
        src = os.path.join(ROOT_DIR, src_rel)
        dest = os.path.join(ARCHIVE_DIR, dest_rel)
        if os.path.exists(src):
            moves.append((src, dest, "legacy"))

    return moves


def execute_moves(moves, dry_run=False):
    """æ‰§è¡Œæ–‡ä»¶ç§»åŠ¨æ“ä½œ"""
    if not moves:
        print("  âœ… æ²¡æœ‰éœ€è¦å½’æ¡£çš„æ–‡ä»¶")
        return 0

    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    by_category = defaultdict(list)
    for src, dest, cat in moves:
        by_category[cat].append((src, dest))

    total = 0
    category_labels = {
        "output": "ğŸ“¦ è¾“å‡ºæ–‡ä»¶å½’æ¡£",
        "trade_data": "ğŸ’° äº¤æ˜“æ•°æ®å½’æ¡£",
        "notebook": "ğŸ““ Legacy Notebook å½’æ¡£",
        "legacy": "ğŸ§¹ æµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶",
    }

    for cat, cat_moves in by_category.items():
        label = category_labels.get(cat, cat)
        print(f"\n{'='*60}")
        print(f"  {label} ({len(cat_moves)} ä¸ªæ–‡ä»¶)")
        print(f"{'='*60}")

        for src, dest in sorted(cat_moves):
            src_rel = os.path.relpath(src, ROOT_DIR)
            dest_rel = os.path.relpath(dest, ROOT_DIR)

            if dry_run:
                print(f"  ğŸ“‹ {src_rel}")
                print(f"     â†’ {dest_rel}")
            else:
                os.makedirs(os.path.dirname(dest), exist_ok=True)
                if os.path.isdir(src):
                    shutil.move(src, dest)
                else:
                    shutil.move(src, dest)
                print(f"  âœ… {src_rel}")
                print(f"     â†’ {dest_rel}")
            total += 1

    return total


def print_summary(moves, dry_run=False):
    """æ‰“å°å½’æ¡£æ±‡æ€»"""
    by_category = defaultdict(int)
    for _, _, cat in moves:
        by_category[cat] += 1

    mode = "[DRY-RUN é¢„è§ˆ]" if dry_run else "[å·²å®Œæˆ]"
    print(f"\n{'='*60}")
    print(f"  ğŸ“Š å½’æ¡£æ±‡æ€» {mode}")
    print(f"{'='*60}")
    for cat, count in sorted(by_category.items()):
        labels = {
            "output": "è¾“å‡ºæ–‡ä»¶",
            "trade_data": "äº¤æ˜“æ•°æ®",
            "notebook": "Legacy Notebooks",
            "legacy": "æµ‹è¯•/å®éªŒé—ç•™",
        }
        print(f"  â€¢ {labels.get(cat, cat)}: {count} ä¸ª")
    print(f"  â€¢ æ€»è®¡: {len(moves)} ä¸ªæ–‡ä»¶")

    if dry_run:
        print(f"\n  ğŸ’¡ ä½¿ç”¨ä¸å¸¦ --dry-run å‚æ•°è¿è¡Œä»¥å®é™…æ‰§è¡Œå½’æ¡£")


def main():
    parser = argparse.ArgumentParser(
        description="QuantPits å¸¦æ—¥æœŸæ–‡ä»¶å½’æ¡£å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python engine/scripts/archive_dated_files.py --dry-run           # é¢„è§ˆæ¨¡å¼
  python engine/scripts/archive_dated_files.py                      # å®é™…å½’æ¡£ï¼ˆè‡ªåŠ¨è¯»å–é”šç‚¹æ—¥æœŸï¼‰
  python engine/scripts/archive_dated_files.py --anchor-date 2026-02-13  # æŒ‡å®šé”šç‚¹æ—¥æœŸ
  python engine/scripts/archive_dated_files.py --include-notebooks  # åŒæ—¶å½’æ¡£ legacy notebooks
  python engine/scripts/archive_dated_files.py --cleanup-legacy     # æ¸…ç†æµ‹è¯•/å®éªŒé—ç•™
  python engine/scripts/archive_dated_files.py --all                # å…¨éƒ¨å½’æ¡£ï¼ˆå« notebooks + legacyï¼‰
        """
    )
    parser.add_argument("--dry-run", action="store_true",
                        help="é¢„è§ˆæ¨¡å¼ï¼Œåªæ˜¾ç¤ºå°†è¦ç§»åŠ¨çš„æ–‡ä»¶ï¼Œä¸å®é™…æ“ä½œ")
    parser.add_argument("--anchor-date", type=str, default=None,
                        help="é”šç‚¹æ—¥æœŸ YYYY-MM-DDï¼ˆé»˜è®¤ä» latest_train_records.json è¯»å–ï¼‰ã€‚"
                             "è¯¥æ—¥æœŸåŠä¹‹åçš„æ–‡ä»¶ä¿ç•™ï¼Œä¹‹å‰çš„å½’æ¡£")
    parser.add_argument("--skip-trade-data", action="store_true",
                        help="è·³è¿‡äº¤æ˜“æ•°æ®å½’æ¡£ï¼ˆbuy/sell_suggestion, model_opinions, trade_detail ç­‰ï¼‰")
    parser.add_argument("--include-notebooks", action="store_true",
                        help="åŒæ—¶å½’æ¡£å·²é‡æ„çš„ legacy notebooks")
    parser.add_argument("--cleanup-legacy", action="store_true",
                        help="æ¸…ç†æµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶ï¼ˆAlpha158_full, compare_results.py ç­‰ï¼‰")
    parser.add_argument("--all", action="store_true",
                        help="å…¨éƒ¨å½’æ¡£ï¼ˆç­‰åŒäº --include-notebooks --cleanup-legacyï¼‰")

    args = parser.parse_args()

    if args.all:
        args.include_notebooks = True
        args.cleanup_legacy = True

    # è·å–é”šç‚¹æ—¥æœŸ
    anchor_date = get_anchor_date(args.anchor_date)

    print(f"{'='*60}")
    print(f"  QuantPits æ–‡ä»¶å½’æ¡£å·¥å…·")
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  æ¨¡å¼: {'ğŸ” DRY-RUN é¢„è§ˆ' if args.dry_run else 'ğŸš€ å®é™…æ‰§è¡Œ'}")
    print(f"  é”šç‚¹æ—¥æœŸ: {anchor_date}")
    print(f"  è§„åˆ™: æ—¥æœŸ < {anchor_date} çš„æ–‡ä»¶å½’æ¡£ï¼Œ>= çš„ä¿ç•™")
    print(f"{'='*60}")

    all_moves = []

    # â”€â”€ å¸¦æ—¥æœŸæ–‡ä»¶å½’æ¡£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nğŸ“‚ æ‰«æå¸¦æ—¥æœŸæ–‡ä»¶...")
    dated_moves = plan_archive(anchor_date)

    if args.skip_trade_data:
        dated_moves = [(s, d, c) for s, d, c in dated_moves if c != "trade_data"]

    all_moves.extend(dated_moves)

    # â”€â”€ Legacy notebooks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.include_notebooks:
        print(f"\nğŸ““ æ‰«æ legacy notebooks...")
        nb_moves = archive_legacy_notebooks(dry_run=args.dry_run)
        all_moves.extend(nb_moves)

    # â”€â”€ æµ‹è¯•/å®éªŒé—ç•™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.cleanup_legacy:
        print(f"\nğŸ§¹ æ‰«ææµ‹è¯•/å®éªŒé—ç•™æ–‡ä»¶...")
        legacy_moves = archive_legacy_items(dry_run=args.dry_run)
        all_moves.extend(legacy_moves)

    # â”€â”€ æ‰§è¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if all_moves:
        execute_moves(all_moves, dry_run=args.dry_run)
        print_summary(all_moves, dry_run=args.dry_run)
    else:
        print(f"\n  âœ… å½“å‰æ²¡æœ‰éœ€è¦å½’æ¡£çš„æ–‡ä»¶ï¼Œä¸€åˆ‡æ•´æ´ï¼")

    return 0


if __name__ == "__main__":
    sys.exit(main())
