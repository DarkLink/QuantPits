#!/usr/bin/env python3
import os
import pandas as pd
import numpy as np
from datetime import datetime
import argparse

import env
os.chdir(env.ROOT_DIR)

def evaluate_health():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(env.ROOT_DIR, "output")
    
    file_20 = os.path.join(output_dir, "rolling_metrics_20.csv")
    file_60 = os.path.join(output_dir, "rolling_metrics_60.csv")
    
    if not os.path.exists(file_20) or not os.path.exists(file_60):
        print("Required rolling metrics CSVs not found! Please run `run_rolling_analysis.py --windows 20 60` first.")
        return
        
    df_20 = pd.read_csv(file_20, parse_dates=['Date']).set_index('Date').sort_index()
    df_60 = pd.read_csv(file_60, parse_dates=['Date']).set_index('Date').sort_index()
    
    # Need at least 60 days of data
    if len(df_60) < 60:
        print("Not enough history in 60-day metrics to generate health report.")
        return
        
    last_date = df_20.index[-1]
    
    alerts = []
    recommendations = []
    
    # 1. Z-Score Anomaly Detection (Friction limits)
    # Compare today's delay cost & slippage against the past 60 days distribution
    recent_60_slip = df_60['Exec_Slippage_Mean'].tail(60)
    current_slip = recent_60_slip.iloc[-1]
    slip_mean = recent_60_slip.mean()
    slip_std = recent_60_slip.std()
    
    if slip_std > 0:
        z_slip = (current_slip - slip_mean) / slip_std
        if z_slip < -2.0: # Highly negative slippage
            alerts.append(f"ğŸ”´ **æ‰§è¡Œæ‘©æ“¦å´©ç›˜**ï¼šè¿‡å»20ä¸ªäº¤æ˜“æ—¥ï¼ŒExecution Slippage éª¤é™è‡³ {current_slip*100:.2f}% (Z-Score: {z_slip:.2f})ã€‚è¯Šæ–­ï¼šå¼€ç›˜ä»·åˆ°æˆäº¤ä»·çš„æ»‘ç‚¹æŸå¤±æå¤§è¶…å‡ºäº†60æ—¥æ­£å¸¸è­¦æˆ’èŒƒå›´ï¼")
            recommendations.append("[å‚æ•°è°ƒæ•´]ï¼šå»ºè®®ç«‹å³å°†ä¹°å–ç‚¹æ‰“å•ç®—æ³•ä»å¸‚ä»·å• (Market/Moo) åˆ‡æ¢ä¸ºé™ä»·å• (Limit) æˆ– TWAP/VWAP ç®—æ³•ï¼Œä»¥è§„é¿ä»·æ ¼æ‰“æ»‘æå€¼ã€‚")
            
    recent_60_delay = df_60['Delay_Cost_Mean'].tail(60)
    current_delay = recent_60_delay.iloc[-1]
    delay_mean = recent_60_delay.mean()
    delay_std = recent_60_delay.std()
    
    if delay_std > 0:
        z_delay = (current_delay - delay_mean) / delay_std
        if z_delay < -2.0:
            alerts.append(f"ğŸ”´ **éš”å¤œè·³ç©ºæ¶åŒ–**ï¼šDelay Cost è·Œè‡³ {current_delay*100:.2f}% (Z-Score: {z_delay:.2f})ã€‚è¯Šæ–­ï¼šTæ—¥æ”¶ç›˜è‡³T+1æ—¥å¼€ç›˜çš„å¤©ç„¶è·³ç©ºç¼ºå£é€ æˆå·¨é¢éšè—æŸå¤±ï¼Œåšå¸‚ç¯å¢ƒæ­£åœ¨æ€¥å‰§æ¶åŒ–ã€‚")
            
    # 2. Moving Average Crossover (Alpha decay)
    # Check if short-term (20d) Idiosyncratic Alpha drops below long-term (60d)
    idio_20 = df_20['Idiosyncratic_Alpha'].tail(5).mean()
    idio_60 = df_60['Idiosyncratic_Alpha'].tail(5).mean()
    
    if idio_20 < idio_60 and idio_20 < 0:
        alerts.append(f"ğŸŸ¡ **Alpha Decay (é˜¿å°”æ³•è¡°å‡)**ï¼šè¿‘5æ—¥ç›‘æ§ç‚¹æ˜¾ç¤ºï¼Œ20æ—¥æ»šåŠ¨çš„çŸ­æœŸç‰¹è´¨Alpha ({idio_20*100:.2f}%) å·²æ˜¾è‘—ä¸‹ç©¿ 60æ—¥å‡çº¿ ({idio_60*100:.2f}%) å¹¶è·Œå…¥è´ŸåŸŸã€‚è¯Šæ–­ï¼šç­–ç•¥çš„åº•å±‚é€‰è‚¡ç½‘ç»œæ­£åœ¨è¿…é€Ÿå¤±æ•ˆï¼")
        recommendations.append("[é£æ§ç†”æ–­]ï¼šçŸ­æœŸé€‰è‚¡è¶…é¢æ”¶ç›Šå‡ºç°æ­»å‰ï¼Œè‹¥æœªæ¥ä¸€å‘¨æ— æ³•ä¿®å¤å›å½’é›¶è½´ï¼Œå»ºè®®å¯åŠ¨é˜²æ´ªæœºåˆ¶é™ä½ç³»ç»Ÿæ•´ä½“è¿ä½œæ æ†æˆ–å¼ºåˆ¶ç©ºä»“ã€‚")
    elif idio_20 > idio_60 and idio_20 > 0:
        alerts.append(f"ğŸŸ¢ **é€‰è‚¡Alphaçˆ†å‘**ï¼š20æ—¥ç‰¹è´¨è¶…é¢ ({idio_20*100:.2f}%) å¼ºåŠ²å‘ä¸Šç©¿è¶Š 60æ—¥æ°´å‡† ({idio_60*100:.2f}%)ã€‚è¯Šæ–­ï¼šå½“å‰åˆ†åŒ–è¡Œæƒ…æ·±åº¦å¥‘åˆå› å­æŒ–æ˜æ± ï¼Œçº¯ç²¹çš„é€‰è‚¡èƒ½åŠ›å‘ˆç°äº•å–·æ€åŠ¿ï¼")

    # 3. Historical Percentile Extremes (Factor Drift)
    # Evaluate 1-year historical percentiles for Barra exposures
    past_year_60 = df_60.last("252D").dropna(subset=['Exposure_Size', 'Exposure_Momentum', 'Exposure_Volatility'])
    
    if not past_year_60.empty:
        curr_size = past_year_60['Exposure_Size'].iloc[-1]
        size_5th = past_year_60['Exposure_Size'].quantile(0.05)
        size_95th = past_year_60['Exposure_Size'].quantile(0.95)
        
        if curr_size <= size_5th:
            alerts.append(f"ğŸ”´ **æç«¯å¾®ç›˜æš´éœ²**ï¼šBarra_Size_Exp æ»šåŠ¨å‡å€¼è·Œè‡³ {curr_size:.2f} (çªç ´ä¸€å¹´å†…5%æä½åˆ†ä½)ã€‚è¯Šæ–­ï¼šç­–ç•¥å·²ä¸¥é‡åç¦»ä¸­ç›˜åŸºå‡†ï¼Œå½“å‰æ­£æš´éœ²äºæé«˜çš„å¾®ç›˜è‚¡æµåŠ¨æ€§é£é™©åŒºï¼")
            recommendations.append("[å› å­å‰¥ç¦»]ï¼šå»ºè®®åœ¨æ¨¡å‹èåˆè®­ç»ƒå±‚é¢åŠ å…¥ Size å› å­ä¸­æ€§åŒ–çº¦æŸï¼Œå¼ºè¡Œæ–©æ–­é’ˆå¯¹æµé€šå¸‚å€¼çš„è¿‡åº¦æ‹¥æŒ¤åå¥½ã€‚")
        elif curr_size >= size_95th:
            alerts.append(f"ğŸ”´ **å¤§ç›˜è‚¡è¶…è½½**ï¼šBarra_Size_Exp æ”€å‡è‡³ {curr_size:.2f} (çªç ´ä¸€å¹´å†…95%æé«˜åˆ†ä½)ã€‚è¯Šæ–­ï¼šé£æ ¼å‘æƒé‡è“ç­¹ä¸¥é‡æ¼‚ç§»ã€‚")

        # Win Rate Trend
        past_20_wr = df_20['Win_Rate'].dropna()
        if len(past_20_wr) >= 20: # Ensure we have at least 20 days to look back on 20d data
             curr_wr = past_20_wr.iloc[-1]
             prev_wr = past_20_wr.iloc[-20]
             
             wr_trend = "â¡ï¸"
             if curr_wr > prev_wr + 0.02: wr_trend = "ğŸ“ˆ ä¸Šå‡"
             elif curr_wr < prev_wr - 0.02: wr_trend = "ğŸ“‰ ä¸‹é™"
             
             pr_curr = df_20['Payoff_Ratio'].dropna().iloc[-1]
             pr_prev = df_20['Payoff_Ratio'].dropna().iloc[-20]
             
             top_holdings = ""
             if "Holdings_Top1_Concentration" in df_20.columns: # Placeholder logic if holding feature expands
                 top_holdings = df_20['Holdings_Top1_Concentration'].iloc[-1]
    
    # Assemble Status
    health_status = "ğŸŸ¢ æ­£å¸¸è¿è½¬ (æ— ç³»ç»Ÿæ€§å±æœº)"
    if len([a for a in alerts if 'ğŸ”´' in a]) >= 2:
         health_status = "ğŸ”´ å±é™© (æ ¸å¿ƒå› å­æ¼‚ç§»æˆ–æ‘©æ“¦å‰§çƒˆé‡ä»“ç§¯èšï¼Œå»ºè®®åœæœºå¹²é¢„)"
    elif len(alerts) > 0:
         health_status = "ğŸŸ¡ è­¦å‘Š (å‡ºç°å¼‚åŠ¨æˆ–è½»å¾®æ‘©æ“¦ä¾µèš€ï¼Œéœ€å…³æ³¨ç›‘æ§)"

    # Format Markdown string
    report_md = f"""# ğŸ“Š ç­–ç•¥å‘¨åº¦æ»šåŠ¨ä½“æ£€æŠ¥å‘Š (Weekly Rolling Health Summary)

**æ—¥æœŸæ ‡ç­¾ï¼š** {last_date.strftime('%Y-%m-%d')} | **å¯¹æ¯”åŸºæœŸï¼š** è¿‡å»20ä¸ªäº¤æ˜“æ—¥ (T-20) 
**æ€»ä½“å¥åº·åº¦ï¼š** {health_status}

### 1. ğŸš¨ æ ¸å¿ƒå¼‚åŠ¨è­¦å‘Š (Anomalies & Alerts)
"""
    if alerts:
        for alert in alerts:
            report_md += f"- {alert}\n"
    else:
        report_md += "- æš‚æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„é£æ ¼æ¼‚ç§»ã€é˜¿å°”æ³•è¡°å‡æˆ–ç»Ÿè®¡çº§æ‘©æ“¦å¼‚å¸¸ã€‚\n"

    report_md += f"""
### 2. ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿ (Performance Trends: 20-Day Rolling)
- **Win Rate (æ—¥å†èƒœç‡):** {prev_wr*100:.2f}% â¡ï¸ {curr_wr*100:.2f}% ({wr_trend})
- **Payoff Ratio (ç›ˆäºæ¯”):** {pr_prev:.2f} â¡ï¸ {pr_curr:.2f}
"""
    
    report_md += """
### 3. ğŸ¤– ç³»ç»Ÿè‡ªåŠ¨åŒ–æ“ä½œå»ºè®® (Actionable Recommendations)
"""
    if recommendations:
        # Deduplicate
        unique_recs = list(set(recommendations))
        for rec in unique_recs:
             report_md += f"- {rec}\n"
    else:
        report_md += "- å½“å‰ç¯å¢ƒè‰¯å¥½ï¼Œå»ºè®®ä¿æŒåŸæœ‰æµæ°´çº¿æƒ¯æ€§è¿è½¬ã€‚\n"

    # Write output
    out_file = os.path.join(output_dir, "rolling_health_report.md")
    with open(out_file, "w", encoding="utf-8") as f:
        f.write(report_md)
        
    print(f"\nâœ… Rolling Health Summary correctly generated at: {out_file}")
    print("="*60)
    print(report_md)
    print("="*60)
    
if __name__ == "__main__":
    evaluate_health()
